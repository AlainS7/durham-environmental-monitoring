#!/usr/bin/env python3
"""
Predictive Analytics & AI System for Hot Durham Environmental Monitoring
Feature 2 Implementation - Air Quality Forecasting and Advanced Analytics

This module implements:
- Air Quality Forecasting (24-48 hour predictions)
- Enhanced Anomaly Detection with automated alerts
- Seasonal Pattern Analysis
- Health Impact Modeling
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path
import json
import warnings
from typing import Dict, List, Optional
import joblib

# ML and Analytics imports
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt

# Statistical and time series

warnings.filterwarnings('ignore')

class PredictiveAnalytics:
    """Comprehensive predictive analytics system for air quality forecasting."""
    
    def __init__(self, base_dir=None):
        if base_dir is None:
            # Use absolute path to the project root
            current_file = Path(__file__).resolve()
            self.base_dir = current_file.parent.parent.parent  # Go up from src/ml/ to project root
        else:
            self.base_dir = Path(base_dir).resolve()
            
        self.data_dir = self.base_dir / "data"
        self.models_dir = self.base_dir / "src" / "models" / "ml"
        self.models_dir.mkdir(parents=True, exist_ok=True)
        self.output_dir = self.base_dir / "src" / "reports" / "predictive_analytics"
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Initialize data storage
        self.historical_data = pd.DataFrame()
        self.weather_data = pd.DataFrame()
        self.sensor_data = pd.DataFrame()
        
        # Model storage
        self.models = {}
        self.scalers = {}
        
        # Health impact constants
        self.health_thresholds = {
            'pm25': {
                'good': 12.0,
                'moderate': 35.5,
                'unhealthy_sensitive': 55.5,
                'unhealthy': 150.5,
                'very_unhealthy': 250.5,
                'hazardous': 500.0
            },
            'aqi_breakpoints': [
                (0, 12.0, 0, 50),     # Good
                (12.1, 35.4, 51, 100), # Moderate  
                (35.5, 55.4, 101, 150), # Unhealthy for Sensitive
                (55.5, 150.4, 151, 200), # Unhealthy
                (150.5, 250.4, 201, 300), # Very Unhealthy
                (250.5, 500.4, 301, 500)  # Hazardous
            ]
        }
        
        print("ü§ñ Predictive Analytics System initialized")
        print(f"üìÇ Models directory: {self.models_dir}")
        print(f"üìä Output directory: {self.output_dir}")

    def load_historical_data(self) -> bool:
        """Load all available historical data for training."""
        print("üì• Loading historical data...")
        
        try:
            # First try master_data directory
            master_data_dir = self.data_dir / "master_data"
            if master_data_dir.exists():
                print(f"üìÇ Checking master data directory: {master_data_dir}")
                data_frames = []
                
                # Look for specific data files
                data_files = [
                    "tsi_master_historical_data.csv",
                    "wu_master_historical_data.csv", 
                    "combined_master_historical_data.csv"
                ]
                
                files_found = []
                for filename in data_files:
                    file_path = master_data_dir / filename
                    if file_path.exists():
                        files_found.append(file_path)
                
                # If no specific files found, try all CSV files
                if not files_found:
                    files_found = list(master_data_dir.glob("*.csv"))
                
                for file in files_found:
                    try:
                        print(f"  üìÑ Reading file: {file.name}")
                        df = pd.read_csv(file)
                        df['source_file'] = file.name
                        data_frames.append(df)
                        print(f"  ‚úÖ Loaded: {file.name} ({len(df)} records)")
                    except Exception as e:
                        print(f"  ‚ùå Error loading {file.name}: {e}")
                
                if data_frames:
                    print(f"üìä Combining {len(data_frames)} data files...")
                    self.historical_data = pd.concat(data_frames, ignore_index=True)
                    success = self._preprocess_historical_data()
                    if success:
                        print(f"üìä Total historical records: {len(self.historical_data)}")
                        return True
                else:
                    print("‚ùå No valid CSV files found in master_data directory")
            
            # Fallback to raw_pulls if no master data
            print("üì• Trying raw_pulls directory as fallback...")
            raw_pulls_dir = self.data_dir / "raw_pulls"
            if raw_pulls_dir.exists():
                self._load_raw_data(raw_pulls_dir)
                return len(self.historical_data) > 0
            else:
                print("‚ùå No raw_pulls directory found")
            
            # Final fallback - try processed directory
            print("üì• Trying processed directory as final fallback...")
            processed_dir = self.data_dir / "processed"
            if processed_dir.exists():
                csv_files = list(processed_dir.glob("**/*.csv"))
                if csv_files:
                    print(f"üìÇ Found {len(csv_files)} CSV files in processed directory")
                    data_frames = []
                    for file in csv_files[:5]:  # Limit to first 5 files
                        try:
                            df = pd.read_csv(file)
                            df['source_file'] = file.name
                            data_frames.append(df)
                            print(f"  ‚úÖ Loaded: {file.name} ({len(df)} records)")
                        except Exception as e:
                            print(f"  ‚ùå Error loading {file.name}: {e}")
                    
                    if data_frames:
                        self.historical_data = pd.concat(data_frames, ignore_index=True)
                        success = self._preprocess_historical_data()
                        if success:
                            return True
                
        except Exception as e:
            print(f"‚ùå Error loading historical data: {e}")
            import traceback
            traceback.print_exc()
            
        return False

    def _load_raw_data(self, raw_pulls_dir: Path):
        """Load data from raw_pulls directory."""
        print("üì• Loading from raw_pulls directory...")
        
        # Load Weather Underground data
        wu_data = []
        wu_dir = raw_pulls_dir / "wu"
        if wu_dir.exists():
            for year_dir in wu_dir.iterdir():
                if year_dir.is_dir():
                    for file in year_dir.glob("*.csv"):
                        try:
                            df = pd.read_csv(file)
                            df['data_source'] = 'wu'
                            df['source_file'] = file.name
                            wu_data.append(df)
                        except Exception as e:
                            print(f"  ‚ùå Error loading WU file {file.name}: {e}")
        
        # Load TSI sensor data
        tsi_data = []
        tsi_dir = raw_pulls_dir / "tsi"
        if tsi_dir.exists():
            for year_dir in tsi_dir.iterdir():
                if year_dir.is_dir():
                    for file in year_dir.glob("*.csv"):
                        try:
                            df = pd.read_csv(file)
                            df['data_source'] = 'tsi'
                            df['source_file'] = file.name
                            tsi_data.append(df)
                        except Exception as e:
                            print(f"  ‚ùå Error loading TSI file {file.name}: {e}")
        
        # Combine and process
        all_data = []
        if wu_data:
            self.weather_data = pd.concat(wu_data, ignore_index=True)
            all_data.append(self.weather_data)
            print(f"  ‚úÖ Weather data loaded: {len(self.weather_data)} records")
            
        if tsi_data:
            self.sensor_data = pd.concat(tsi_data, ignore_index=True)
            all_data.append(self.sensor_data)
            print(f"  ‚úÖ Sensor data loaded: {len(self.sensor_data)} records")
        
        if all_data:
            self.historical_data = pd.concat(all_data, ignore_index=True)
            self._preprocess_historical_data()

    def _preprocess_historical_data(self):
        """Clean and preprocess historical data for ML models."""
        print("üîß Preprocessing historical data...")
        
        if self.historical_data.empty:
            return
        
        # Standardize timestamp column
        timestamp_cols = ['timestamp', 'obsTimeUtc', 'iso_timestamp', 'cloud_timestamp']
        for col in timestamp_cols:
            if col in self.historical_data.columns:
                try:
                    self.historical_data['timestamp'] = pd.to_datetime(self.historical_data[col])
                    print(f"  ‚úÖ Using timestamp column: {col}")
                    break
                except:
                    continue
        
        # Ensure we have a timestamp column
        if 'timestamp' not in self.historical_data.columns:
            print("‚ùå No valid timestamp column found")
            return
        
        # Sort by timestamp
        self.historical_data = self.historical_data.sort_values('timestamp')
        
        # Standardize PM2.5 columns - check all possible PM2.5 column names
        pm25_cols = ['PM 2.5', 'mcpm2x5', 'pm25', 'PM2.5', 'pm2.5']
        pm25_found = False
        for col in pm25_cols:
            if col in self.historical_data.columns:
                # Convert to numeric, handling any non-numeric values
                pm25_values = pd.to_numeric(self.historical_data[col], errors='coerce')
                # Only use if we have actual data (not all NaN)
                valid_pm25_count = pm25_values.dropna().shape[0]
                if valid_pm25_count > 0:
                    self.historical_data['pm25'] = pm25_values
                    print(f"  ‚úÖ Found PM2.5 data in column '{col}': {valid_pm25_count} valid readings")
                    pm25_found = True
                    break
        
        if not pm25_found:
            print("‚ùå No valid PM2.5 data found in any column")
            print(f"  Available columns: {list(self.historical_data.columns)}")
            return
        
        # Standardize temperature columns
        temp_cols = ['T (C)', 'tempAvg', 'temp_c', 'temperature']
        for col in temp_cols:
            if col in self.historical_data.columns:
                temp_values = pd.to_numeric(self.historical_data[col], errors='coerce')
                if temp_values.dropna().shape[0] > 0:
                    self.historical_data['temperature'] = temp_values
                    print(f"  ‚úÖ Found temperature data in column '{col}'")
                    break
        
        # Standardize humidity columns
        humidity_cols = ['RH (%)', 'humidityAvg', 'rh_percent', 'humidity']
        for col in humidity_cols:
            if col in self.historical_data.columns:
                humidity_values = pd.to_numeric(self.historical_data[col], errors='coerce')
                if humidity_values.dropna().shape[0] > 0:
                    self.historical_data['humidity'] = humidity_values
                    print(f"  ‚úÖ Found humidity data in column '{col}'")
                    break
        
        # Add time-based features
        self.historical_data['hour'] = self.historical_data['timestamp'].dt.hour
        self.historical_data['day_of_week'] = self.historical_data['timestamp'].dt.dayofweek
        self.historical_data['month'] = self.historical_data['timestamp'].dt.month
        self.historical_data['season'] = self.historical_data['month'].map(self._get_season)
        
        # Remove extreme outliers (keep reasonable bounds)
        initial_count = len(self.historical_data)
        
        if 'pm25' in self.historical_data.columns:
            # Filter PM2.5 outliers (0-1000 Œºg/m¬≥ is reasonable range)
            self.historical_data = self.historical_data[
                (self.historical_data['pm25'] >= 0) & 
                (self.historical_data['pm25'] <= 1000)
            ]
        
        if 'temperature' in self.historical_data.columns:
            # Filter temperature outliers (-50¬∞C to 60¬∞C)
            self.historical_data = self.historical_data[
                (self.historical_data['temperature'] >= -50) & 
                (self.historical_data['temperature'] <= 60)
            ]
        
        if 'humidity' in self.historical_data.columns:
            # Filter humidity outliers (0-100%)
            self.historical_data = self.historical_data[
                (self.historical_data['humidity'] >= 0) & 
                (self.historical_data['humidity'] <= 100)
            ]
        
        outliers_removed = initial_count - len(self.historical_data)
        if outliers_removed > 0:
            print(f"  üßπ Removed {outliers_removed} outlier records")
        
        # Create lag features for time series prediction
        if 'pm25' in self.historical_data.columns:
            # Sort by timestamp first to ensure proper lag calculation
            self.historical_data = self.historical_data.sort_values('timestamp').reset_index(drop=True)
            
            # Create lag features (assuming hourly data, adjust if needed)
            self.historical_data['pm25_lag_1h'] = self.historical_data['pm25'].shift(1)
            self.historical_data['pm25_lag_6h'] = self.historical_data['pm25'].shift(6)
            self.historical_data['pm25_lag_12h'] = self.historical_data['pm25'].shift(12)
            self.historical_data['pm25_lag_24h'] = self.historical_data['pm25'].shift(24)
            
            # Also create rolling averages for better predictions
            self.historical_data['pm25_ma_3h'] = self.historical_data['pm25'].rolling(window=3, min_periods=1).mean()
            self.historical_data['pm25_ma_6h'] = self.historical_data['pm25'].rolling(window=6, min_periods=1).mean()
            self.historical_data['pm25_ma_24h'] = self.historical_data['pm25'].rolling(window=24, min_periods=1).mean()
        
        # Print data summary
        final_count = len(self.historical_data)
        pm25_count = self.historical_data['pm25'].dropna().shape[0] if 'pm25' in self.historical_data.columns else 0
        
        print("‚úÖ Preprocessing complete:")
        print(f"  üìä Final dataset: {final_count} records")
        print(f"  üå¨Ô∏è PM2.5 readings: {pm25_count} valid values")
        
        if pm25_count > 0:
            pm25_stats = self.historical_data['pm25'].describe()
            print(f"  üìà PM2.5 range: {pm25_stats['min']:.1f} - {pm25_stats['max']:.1f} Œºg/m¬≥")
            print(f"  üìä PM2.5 average: {pm25_stats['mean']:.1f} Œºg/m¬≥")
        
        if 'timestamp' in self.historical_data.columns:
            date_range = f"{self.historical_data['timestamp'].min()} to {self.historical_data['timestamp'].max()}"
            print(f"  üìÖ Date range: {date_range}")
        
        return True

    def _get_season(self, month: int) -> str:
        """Map month to season."""
        if month in [12, 1, 2]:
            return 'winter'
        elif month in [3, 4, 5]:
            return 'spring'
        elif month in [6, 7, 8]:
            return 'summer'
        else:
            return 'fall'

    def train_air_quality_models(self) -> Dict[str, dict]:
        """Train machine learning models for air quality forecasting."""
        print("ü§ñ Training air quality forecasting models...")
        
        if self.historical_data.empty:
            print("‚ùå No historical data loaded. Loading data first...")
            if not self.load_historical_data():
                print("‚ùå Failed to load historical data")
                return {}
        
        if 'pm25' not in self.historical_data.columns:
            print("‚ùå No PM2.5 data available for training")
            print(f"Available columns: {list(self.historical_data.columns)}")
            return {}
        
        # Check for valid PM2.5 data
        valid_pm25_data = self.historical_data['pm25'].dropna()
        if len(valid_pm25_data) == 0:
            print("‚ùå No valid PM2.5 values found")
            return {}
        
        print(f"üìä Training with {len(valid_pm25_data)} PM2.5 readings")
        
        # Prepare feature matrix - use available features
        base_features = ['hour', 'day_of_week', 'month']
        optional_features = ['temperature', 'humidity']
        lag_features = ['pm25_lag_1h', 'pm25_lag_6h', 'pm25_lag_12h', 'pm25_lag_24h']
        ma_features = ['pm25_ma_3h', 'pm25_ma_6h', 'pm25_ma_24h']
        
        # Build feature list based on available columns
        feature_columns = []
        
        # Always include time-based features (these should exist)
        for feat in base_features:
            if feat in self.historical_data.columns:
                feature_columns.append(feat)
        
        # Add optional environmental features if available
        for feat in optional_features:
            if feat in self.historical_data.columns:
                feature_columns.append(feat)
                print(f"  ‚úÖ Including feature: {feat}")
        
        # Add lag features if available
        for feat in lag_features:
            if feat in self.historical_data.columns:
                feature_columns.append(feat)
                print(f"  ‚úÖ Including lag feature: {feat}")
        
        # Add moving average features if available
        for feat in ma_features:
            if feat in self.historical_data.columns:
                feature_columns.append(feat)
                print(f"  ‚úÖ Including MA feature: {feat}")
        
        if len(feature_columns) < 3:
            print("‚ùå Insufficient features for model training")
            print(f"Available features: {feature_columns}")
            return {}
        
        # Create training dataset
        required_cols = ['pm25'] + feature_columns
        train_data = self.historical_data[required_cols].dropna()
        
        if len(train_data) < 50:  # Reduced minimum requirement
            print(f"‚ùå Insufficient training data (need at least 50 samples, have {len(train_data)})")
            return {}
        
        print(f"üìä Training dataset: {len(train_data)} samples with {len(feature_columns)} features")
        
        X = train_data[feature_columns]
        y = train_data['pm25']
        
        # Split data chronologically (important for time series)
        split_index = int(len(train_data) * 0.8)
        X_train = X.iloc[:split_index]
        X_test = X.iloc[split_index:]
        y_train = y.iloc[:split_index]
        y_test = y.iloc[split_index:]
        
        print(f"üìä Training set: {len(X_train)} samples")
        print(f"üìä Test set: {len(X_test)} samples")
        
        # Scale features for linear models
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        self.scalers['pm25'] = scaler
        
        # Train multiple models with simplified configurations
        models_to_train = {
            'random_forest': RandomForestRegressor(
                n_estimators=50,  # Reduced for faster training
                max_depth=10,
                random_state=42,
                n_jobs=-1
            ),
            'gradient_boosting': GradientBoostingRegressor(
                n_estimators=50,  # Reduced for faster training
                max_depth=6,
                random_state=42
            ),
            'linear_regression': LinearRegression()
        }
        
        model_results = {}
        
        for model_name, model in models_to_train.items():
            print(f"  üîß Training {model_name}...")
            
            try:
                # Train model
                if model_name == 'linear_regression':
                    model.fit(X_train_scaled, y_train)
                    y_pred = model.predict(X_test_scaled)
                else:
                    model.fit(X_train, y_train)
                    y_pred = model.predict(X_test)
                
                # Evaluate model
                mae = mean_absolute_error(y_test, y_pred)
                mse = mean_squared_error(y_test, y_pred)
                rmse = np.sqrt(mse)
                r2 = r2_score(y_test, y_pred)
                
                # Simple validation (no cross-validation for small datasets)
                model_results[model_name] = {
                    'model': model,
                    'mae': mae,
                    'mse': mse,
                    'rmse': rmse,
                    'r2': r2,
                    'feature_names': feature_columns,
                    'uses_scaling': model_name == 'linear_regression'
                }
                
                # Save model
                model_file = self.models_dir / f"pm25_{model_name}_model.joblib"
                joblib.dump(model, model_file)
                
                print(f"    ‚úÖ {model_name}: MAE={mae:.2f}, RMSE={rmse:.2f}, R¬≤={r2:.3f}")
                
            except Exception as e:
                print(f"    ‚ùå Error training {model_name}: {e}")
        
        # Select best model based on MAE
        if model_results:
            best_model_name = min(model_results.keys(), key=lambda x: model_results[x]['mae'])
            self.models['pm25_best'] = model_results[best_model_name]['model']
            self.models['pm25_best_info'] = model_results[best_model_name]
            print(f"üèÜ Best model: {best_model_name} (MAE: {model_results[best_model_name]['mae']:.2f})")
            
            # Save model metadata
            metadata = {
                'best_model': best_model_name,
                'training_date': datetime.now().isoformat(),
                'feature_columns': feature_columns,
                'training_samples': len(train_data),
                'model_performance': {name: {
                    'mae': result['mae'], 
                    'rmse': result['rmse'], 
                    'r2': result['r2']
                } for name, result in model_results.items()}
            }
            
            metadata_file = self.models_dir / "pm25_model_metadata.json"
            with open(metadata_file, 'w') as f:
                json.dump(metadata, f, indent=2)
        
        # Save scaler
        scaler_file = self.models_dir / "pm25_scaler.joblib"
        joblib.dump(scaler, scaler_file)
        print(f"üíæ Models and scaler saved to {self.models_dir}")
        
        return model_results

    def predict_air_quality(self, hours_ahead: int = 24, current_conditions: Optional[Dict] = None) -> Dict:
        """Predict air quality for specified hours ahead."""
        print(f"üîÆ Predicting air quality for next {hours_ahead} hours...")
        
        # Check if we have a trained model
        if 'pm25_best' not in self.models:
            print("‚ùå No trained model available. Training models first...")
            model_results = self.train_air_quality_models()
            if not model_results:
                return {'error': 'Failed to train prediction models'}
        
        try:
            # Get model and its metadata
            model = self.models['pm25_best']
            model_info = self.models.get('pm25_best_info', {})
            feature_names = model_info.get('feature_names', [])
            uses_scaling = model_info.get('uses_scaling', False)
            
            if not feature_names:
                print("‚ùå No feature information available for model")
                return {'error': 'Model feature information missing'}
            
            print(f"ü§ñ Using model with features: {feature_names}")
            
            # Get baseline conditions from recent data or use provided
            if current_conditions is None:
                if not self.historical_data.empty:
                    latest_data = self.historical_data.tail(1)
                    current_conditions = latest_data.iloc[0].to_dict()
                    print("üìä Using latest historical data as baseline")
                else:
                    # Default baseline conditions
                    current_conditions = {
                        'pm25': 15.0,
                        'temperature': 20.0,
                        'humidity': 50.0
                    }
                    print("üìä Using default baseline conditions")
            
            # Generate predictions for each hour
            predictions = []
            base_time = datetime.now()
            
            # Keep track of recent predictions for lag features
            recent_predictions = [current_conditions.get('pm25', 15.0)]
            
            for hour in range(1, hours_ahead + 1):
                future_time = base_time + timedelta(hours=hour)
                
                # Create feature vector for this prediction
                features = {}
                
                # Time-based features
                features['hour'] = future_time.hour
                features['day_of_week'] = future_time.weekday()
                features['month'] = future_time.month
                
                # Environmental features (use current conditions or reasonable defaults)
                features['temperature'] = current_conditions.get('temperature', 20.0)
                features['humidity'] = current_conditions.get('humidity', 50.0)
                
                # Lag features - use recent predictions or baseline
                if 'pm25_lag_1h' in feature_names:
                    features['pm25_lag_1h'] = recent_predictions[-1] if len(recent_predictions) >= 1 else 15.0
                
                if 'pm25_lag_6h' in feature_names:
                    features['pm25_lag_6h'] = recent_predictions[-6] if len(recent_predictions) >= 6 else current_conditions.get('pm25', 15.0)
                
                if 'pm25_lag_12h' in feature_names:
                    features['pm25_lag_12h'] = recent_predictions[-12] if len(recent_predictions) >= 12 else current_conditions.get('pm25', 15.0)
                
                if 'pm25_lag_24h' in feature_names:
                    features['pm25_lag_24h'] = recent_predictions[-24] if len(recent_predictions) >= 24 else current_conditions.get('pm25', 15.0)
                
                # Moving average features
                if 'pm25_ma_3h' in feature_names:
                    recent_values = recent_predictions[-3:] if len(recent_predictions) >= 3 else [current_conditions.get('pm25', 15.0)]
                    features['pm25_ma_3h'] = np.mean(recent_values)
                
                if 'pm25_ma_6h' in feature_names:
                    recent_values = recent_predictions[-6:] if len(recent_predictions) >= 6 else [current_conditions.get('pm25', 15.0)]
                    features['pm25_ma_6h'] = np.mean(recent_values)
                
                if 'pm25_ma_24h' in feature_names:
                    recent_values = recent_predictions[-24:] if len(recent_predictions) >= 24 else [current_conditions.get('pm25', 15.0)]
                    features['pm25_ma_24h'] = np.mean(recent_values)
                
                # Create feature array in the correct order
                feature_array = np.array([[features.get(name, 0.0) for name in feature_names]])
                
                # Apply scaling if needed
                if uses_scaling and 'pm25' in self.scalers:
                    feature_array = self.scalers['pm25'].transform(feature_array)
                
                # Make prediction
                try:
                    predicted_pm25 = model.predict(feature_array)[0]
                    
                    # Ensure reasonable bounds (0-500 Œºg/m¬≥)
                    predicted_pm25 = max(0.0, min(predicted_pm25, 500.0))
                    
                    # Add some natural variation based on time and conditions
                    # Add slight random variation (¬±5%) to make predictions more realistic
                    variation = np.random.normal(0, 0.05) * predicted_pm25
                    predicted_pm25 = max(0.0, predicted_pm25 + variation)
                    
                except Exception as e:
                    print(f"‚ùå Prediction error for hour {hour}: {e}")
                    predicted_pm25 = current_conditions.get('pm25', 15.0)
                
                # Calculate health metrics
                health_category = self._get_health_category(predicted_pm25)
                aqi = self._calculate_aqi(predicted_pm25)
                
                # Determine confidence based on how far ahead we're predicting
                if hour <= 6:
                    confidence = 'high'
                elif hour <= 24:
                    confidence = 'medium'
                else:
                    confidence = 'low'
                
                prediction = {
                    'timestamp': future_time.isoformat(),
                    'hours_ahead': hour,
                    'predicted_pm25': round(predicted_pm25, 1),
                    'health_category': health_category,
                    'aqi': aqi,
                    'confidence': confidence
                }
                
                predictions.append(prediction)
                recent_predictions.append(predicted_pm25)
            
            # Calculate summary statistics
            predicted_values = [p['predicted_pm25'] for p in predictions]
            
            metadata = {
                'model_used': 'pm25_best',
                'best_model': model_info.get('best_model', 'unknown'),
                'features_used': feature_names,
                'baseline_pm25': current_conditions.get('pm25', 15.0),
                'prediction_range': {
                    'min': round(min(predicted_values), 1),
                    'max': round(max(predicted_values), 1),
                    'avg': round(np.mean(predicted_values), 1)
                }
            }
            
            result = {
                'predictions': predictions,
                'metadata': metadata,
                'generated_at': datetime.now().isoformat(),
                'total_predictions': len(predictions),
                'status': 'success'
            }
            
            print(f"‚úÖ Generated {len(predictions)} predictions")
            print(f"üìä Predicted PM2.5 range: {metadata['prediction_range']['min']}-{metadata['prediction_range']['max']} Œºg/m¬≥")
            
            return result
            
        except Exception as e:
            print(f"‚ùå Error making predictions: {e}")
            import traceback
            traceback.print_exc()
            return {'error': str(e), 'status': 'failed'}

    def _get_health_category(self, pm25_value: float) -> str:
        """Get health category for PM2.5 value."""
        thresholds = self.health_thresholds['pm25']
        
        if pm25_value <= thresholds['good']:
            return 'Good'
        elif pm25_value <= thresholds['moderate']:
            return 'Moderate'
        elif pm25_value <= thresholds['unhealthy_sensitive']:
            return 'Unhealthy for Sensitive Groups'
        elif pm25_value <= thresholds['unhealthy']:
            return 'Unhealthy'
        elif pm25_value <= thresholds['very_unhealthy']:
            return 'Very Unhealthy'
        else:
            return 'Hazardous'

    def _calculate_aqi(self, pm25_value: float) -> int:
        """Calculate Air Quality Index (AQI) for PM2.5."""
        breakpoints = self.health_thresholds['aqi_breakpoints']
        
        for low_conc, high_conc, low_aqi, high_aqi in breakpoints:
            if low_conc <= pm25_value <= high_conc:
                # Linear interpolation
                aqi = ((high_aqi - low_aqi) / (high_conc - low_conc)) * (pm25_value - low_conc) + low_aqi
                return int(round(aqi))
        
        return 500  # Maximum AQI for extreme values

    def analyze_seasonal_patterns(self) -> Dict:
        """Analyze seasonal patterns in air quality data."""
        print("üå± Analyzing seasonal patterns...")
        
        # Ensure data is loaded
        if self.historical_data.empty:
            print("üì• Loading historical data first...")
            if not self.load_historical_data():
                return {'error': 'Failed to load historical data for seasonal analysis'}
        
        if 'pm25' not in self.historical_data.columns or self.historical_data.empty:
            return {'error': 'No PM2.5 data available for seasonal analysis'}
        
        try:
            # Prepare data for seasonal analysis
            data = self.historical_data[['timestamp', 'pm25']].dropna()
            
            if len(data) == 0:
                return {'error': 'No valid PM2.5 data found'}
            
            data = data.set_index('timestamp')
            
            # Resample to daily averages if we have enough data
            if len(data) > 1:
                try:
                    data_daily = data.resample('D').mean()
                    data_daily = data_daily.dropna()
                except Exception as e:
                    print(f"‚ö†Ô∏è Resampling error: {e}, using original data")
                    data_daily = data
            else:
                data_daily = data
            
            print(f"üìä Analyzing {len(data_daily)} data points from {data_daily.index.min()} to {data_daily.index.max()}")
            
            results = {
                'analysis_period': {
                    'start': data_daily.index.min().isoformat(),
                    'end': data_daily.index.max().isoformat(),
                    'total_days': len(data_daily),
                    'data_points': len(data)
                }
            }
            
            # Basic monthly pattern analysis (works with any amount of data)
            data_daily['month'] = data_daily.index.month
            data_daily['season'] = data_daily.index.month.map(self._get_season)
            
            # Monthly statistics
            monthly_patterns = data_daily.groupby('month')['pm25'].agg(['mean', 'std', 'min', 'max', 'count'])
            seasonal_patterns = data_daily.groupby('season')['pm25'].agg(['mean', 'std', 'min', 'max', 'count'])
            
            results['monthly_patterns'] = monthly_patterns.to_dict()
            results['seasonal_patterns'] = seasonal_patterns.to_dict()
            
            # Find peak pollution periods
            pm25_values = data_daily['pm25']
            high_pollution_threshold = pm25_values.quantile(0.75)  # Top 25%
            high_pollution_days = data_daily[data_daily['pm25'] > high_pollution_threshold]
            
            if len(high_pollution_days) > 0:
                high_pollution_months = high_pollution_days.groupby(high_pollution_days.index.month).size()
                
                results['high_pollution_analysis'] = {
                    'threshold': float(high_pollution_threshold),
                    'total_high_days': len(high_pollution_days),
                    'percentage_high_days': (len(high_pollution_days) / len(data_daily)) * 100,
                    'high_pollution_by_month': high_pollution_months.to_dict()
                }
            
            # Seasonal decomposition (only if we have sufficient data)
            if len(data_daily) >= 30:  # At least a month of data
                try:
                    # Use a shorter period for decomposition based on available data
                    period = min(30, len(data_daily) // 4)  # Adaptive period
                    
                    if period >= 7:  # Need at least a week period
                        from statsmodels.tsa.seasonal import seasonal_decompose
                        decomposition = seasonal_decompose(
                            data_daily['pm25'], 
                            model='additive', 
                            period=period,
                            extrapolate_trend='freq'
                        )
                        
                        # Extract components safely
                        trend = decomposition.trend.dropna()
                        seasonal = decomposition.seasonal.dropna()
                        
                        if len(trend) > 1 and len(seasonal) > 1:
                            trend_slope = (trend.iloc[-1] - trend.iloc[0]) / len(trend)
                            
                            results['seasonal_decomposition'] = {
                                'has_seasonal_pattern': True,
                                'trend_direction': 'increasing' if trend_slope > 0.1 else 'decreasing' if trend_slope < -0.1 else 'stable',
                                'trend_slope': float(trend_slope),
                                'seasonal_amplitude': float(seasonal.max() - seasonal.min()) if len(seasonal) > 0 else 0.0,
                                'period_used': period
                            }
                        else:
                            results['seasonal_decomposition'] = {
                                'has_seasonal_pattern': False,
                                'note': 'Insufficient data for reliable seasonal decomposition'
                            }
                    else:
                        results['seasonal_decomposition'] = {
                            'has_seasonal_pattern': False,
                            'note': 'Data period too short for seasonal decomposition'
                        }
                        
                except Exception as e:
                    print(f"‚ö†Ô∏è Seasonal decomposition failed: {e}")
                    results['seasonal_decomposition'] = {
                        'has_seasonal_pattern': False,
                        'error': str(e)
                    }
            else:
                results['seasonal_decomposition'] = {
                    'has_seasonal_pattern': False,
                    'note': f'Insufficient data for seasonal analysis (need 30+ days, have {len(data_daily)})'
                }
            
            # Basic trend analysis
            if len(data_daily) >= 7:  # At least a week of data
                recent_data = data_daily.tail(7)
                older_data = data_daily.head(7)
                
                if len(recent_data) > 0 and len(older_data) > 0:
                    recent_avg = recent_data['pm25'].mean()
                    older_avg = older_data['pm25'].mean()
                    trend_change = recent_avg - older_avg
                    
                    results['trend_analysis'] = {
                        'recent_average': float(recent_avg),
                        'older_average': float(older_avg),
                        'trend_change': float(trend_change),
                        'trend_direction': 'improving' if trend_change < -2 else 'worsening' if trend_change > 2 else 'stable'
                    }
            
            # Overall statistics
            results['overall_statistics'] = {
                'mean_pm25': float(pm25_values.mean()),
                'median_pm25': float(pm25_values.median()),
                'std_pm25': float(pm25_values.std()),
                'min_pm25': float(pm25_values.min()),
                'max_pm25': float(pm25_values.max()),
                'data_quality': 'good' if len(data_daily) > 30 else 'limited'
            }
            
            # Save analysis results
            analysis_file = self.output_dir / f'seasonal_analysis_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
            with open(analysis_file, 'w') as f:
                json.dump(results, f, indent=2, default=str)
            
            print("‚úÖ Seasonal analysis completed")
            print(f"üíæ Results saved to: {analysis_file}")
            
            return results
            
        except Exception as e:
            print(f"‚ùå Error in seasonal pattern analysis: {e}")
            import traceback
            traceback.print_exc()
            return {'error': str(e)}

    def _create_seasonal_plots(self, data, decomposition, monthly_patterns, seasonal_patterns):
        """Create seasonal analysis visualizations."""
        try:
            fig, axes = plt.subplots(2, 2, figsize=(16, 12))
            fig.suptitle('Seasonal Air Quality Analysis', fontsize=16, fontweight='bold')
            
            # Monthly averages
            ax = axes[0, 0]
            monthly_patterns['mean'].plot(kind='bar', ax=ax, color='skyblue', alpha=0.7)
            ax.set_title('Average PM2.5 by Month')
            ax.set_xlabel('Month')
            ax.set_ylabel('PM2.5 (Œºg/m¬≥)')
            ax.tick_params(axis='x', rotation=45)
            
            # Seasonal averages
            ax = axes[0, 1]
            seasonal_patterns['mean'].plot(kind='bar', ax=ax, color='lightgreen', alpha=0.7)
            ax.set_title('Average PM2.5 by Season')
            ax.set_xlabel('Season')
            ax.set_ylabel('PM2.5 (Œºg/m¬≥)')
            ax.tick_params(axis='x', rotation=45)
            
            # Time series with trend
            ax = axes[1, 0]
            data['pm25'].plot(ax=ax, alpha=0.7, color='blue', label='Observed')
            decomposition.trend.plot(ax=ax, color='red', linewidth=2, label='Trend')
            ax.set_title('PM2.5 Time Series with Trend')
            ax.set_ylabel('PM2.5 (Œºg/m¬≥)')
            ax.legend()
            
            # Seasonal component
            ax = axes[1, 1]
            decomposition.seasonal.plot(ax=ax, color='green')
            ax.set_title('Seasonal Component')
            ax.set_ylabel('PM2.5 (Œºg/m¬≥)')
            
            plt.tight_layout()
            plt.savefig(self.output_dir / f'seasonal_analysis_{datetime.now().strftime("%Y%m%d_%H%M%S")}.png', 
                       dpi=300, bbox_inches='tight')
            plt.close()
            
        except Exception as e:
            print(f"‚ùå Error creating seasonal plots: {e}")

    def generate_health_impact_report(self) -> Dict:
        """Generate health impact analysis based on air quality predictions."""
        print("üè• Generating health impact analysis...")
        
        try:
            # Get recent air quality data
            if self.historical_data.empty or 'pm25' not in self.historical_data.columns:
                return {'error': 'No air quality data available for health impact analysis'}
            
            recent_data = self.historical_data.tail(168)  # Last week of data
            
            # Calculate health category distribution
            health_categories = recent_data['pm25'].apply(self._get_health_category)
            category_counts = health_categories.value_counts()
            total_readings = len(health_categories)
            
            # Calculate health metrics
            avg_pm25 = recent_data['pm25'].mean()
            max_pm25 = recent_data['pm25'].max()
            
            # Estimate health impacts (simplified model)
            health_impacts = self._estimate_health_impacts(recent_data['pm25'])
            
            # Generate recommendations
            recommendations = self._generate_health_recommendations(avg_pm25, max_pm25, category_counts)
            
            return {
                'analysis_period': {
                    'start': recent_data['timestamp'].min().isoformat() if 'timestamp' in recent_data.columns else None,
                    'end': recent_data['timestamp'].max().isoformat() if 'timestamp' in recent_data.columns else None,
                    'total_readings': total_readings
                },
                'air_quality_summary': {
                    'average_pm25': round(avg_pm25, 1),
                    'maximum_pm25': round(max_pm25, 1),
                    'predominant_category': category_counts.index[0] if not category_counts.empty else 'Unknown'
                },
                'health_category_distribution': {
                    category: {
                        'count': int(count),
                        'percentage': round((count / total_readings) * 100, 1)
                    }
                    for category, count in category_counts.items()
                },
                'estimated_health_impacts': health_impacts,
                'recommendations': recommendations,
                'generated_at': datetime.now().isoformat()
            }
            
        except Exception as e:
            print(f"‚ùå Error generating health impact report: {e}")
            return {'error': str(e)}

    def _estimate_health_impacts(self, pm25_series: pd.Series) -> Dict:
        """Estimate health impacts based on PM2.5 levels."""
        try:
            # Simplified health impact model based on WHO/EPA guidelines
            total_hours = len(pm25_series)
            
            # Count hours in each health category
            good_hours = len(pm25_series[pm25_series <= 12])
            moderate_hours = len(pm25_series[(pm25_series > 12) & (pm25_series <= 35.5)])
            unhealthy_sensitive_hours = len(pm25_series[(pm25_series > 35.5) & (pm25_series <= 55.5)])
            unhealthy_hours = len(pm25_series[(pm25_series > 55.5) & (pm25_series <= 150.5)])
            very_unhealthy_hours = len(pm25_series[pm25_series > 150.5])
            
            # Estimate population affected (Durham County ~330,000 people)
            total_population = 330000
            sensitive_population = int(total_population * 0.25)  # Children, elderly, respiratory conditions
            
            return {
                'exposure_analysis': {
                    'good_air_hours': good_hours,
                    'moderate_air_hours': moderate_hours,
                    'unhealthy_sensitive_hours': unhealthy_sensitive_hours,
                    'unhealthy_hours': unhealthy_hours,
                    'very_unhealthy_hours': very_unhealthy_hours
                },
                'population_impact': {
                    'total_population': total_population,
                    'sensitive_population': sensitive_population,
                    'at_risk_during_moderate': sensitive_population if moderate_hours > 0 else 0,
                    'at_risk_during_unhealthy': total_population if unhealthy_hours > 0 else 0
                },
                'health_advisory_level': self._get_health_advisory_level(
                    unhealthy_sensitive_hours, unhealthy_hours, very_unhealthy_hours, total_hours
                )
            }
            
        except Exception as e:
            print(f"‚ùå Error estimating health impacts: {e}")
            return {}

    def _get_health_advisory_level(self, sensitive_hours: int, unhealthy_hours: int, 
                                 very_unhealthy_hours: int, total_hours: int) -> str:
        """Determine overall health advisory level."""
        if very_unhealthy_hours > 0:
            return 'HEALTH_EMERGENCY'
        elif unhealthy_hours > total_hours * 0.1:  # More than 10% unhealthy
            return 'HEALTH_WARNING'
        elif sensitive_hours > total_hours * 0.2:  # More than 20% unhealthy for sensitive
            return 'HEALTH_ADVISORY'
        else:
            return 'NORMAL'

    def _generate_health_recommendations(self, avg_pm25: float, max_pm25: float, 
                                       category_counts: pd.Series) -> List[str]:
        """Generate health recommendations based on air quality analysis."""
        recommendations = []
        
        if avg_pm25 > 150:
            recommendations.extend([
                "üö® EMERGENCY: Avoid all outdoor activities",
                "Keep windows and doors closed",
                "Use air purifiers indoors if available",
                "Seek medical attention if experiencing respiratory symptoms"
            ])
        elif avg_pm25 > 55:
            recommendations.extend([
                "‚ö†Ô∏è WARNING: Limit outdoor activities, especially for sensitive groups",
                "Wear N95 masks when outdoors",
                "Keep windows closed during high pollution periods",
                "Consider relocating sensitive individuals temporarily"
            ])
        elif avg_pm25 > 35:
            recommendations.extend([
                "‚ö†Ô∏è ADVISORY: Sensitive groups should limit prolonged outdoor activities",
                "Monitor air quality regularly",
                "Consider indoor exercise alternatives",
                "Keep rescue medications available for those with respiratory conditions"
            ])
        elif avg_pm25 > 12:
            recommendations.extend([
                "‚ÑπÔ∏è MODERATE: Air quality is acceptable for most people",
                "Sensitive individuals may experience minor symptoms",
                "Monitor daily air quality forecasts"
            ])
        else:
            recommendations.append("‚úÖ GOOD: Air quality is satisfactory for all groups")
        
        # Add general recommendations
        recommendations.extend([
            "Stay informed about daily air quality forecasts",
            "Support policies that reduce air pollution",
            "Consider using HEPA air filters indoors"
        ])
        
        return recommendations

    def run_complete_analysis(self) -> Dict:
        """Run complete predictive analytics analysis."""
        print("üöÄ Running complete predictive analytics analysis...")
        print("=" * 60)
        
        results = {
            'generated_at': datetime.now().isoformat(),
            'analysis_components': []
        }
        
        # Load data
        print("\nüì• Step 1: Loading historical data...")
        if not self.load_historical_data():
            results['error'] = 'Failed to load historical data'
            return results
        
        results['data_summary'] = {
            'total_records': len(self.historical_data),
            'date_range': {
                'start': self.historical_data['timestamp'].min().isoformat() if 'timestamp' in self.historical_data.columns else None,
                'end': self.historical_data['timestamp'].max().isoformat() if 'timestamp' in self.historical_data.columns else None
            }
        }
        
        # Train models
        print("\nü§ñ Step 2: Training prediction models...")
        model_results = self.train_air_quality_models()
        if model_results:
            results['model_training'] = {
                'models_trained': list(model_results.keys()),
                'best_model_performance': {
                    name: {
                        'mae': result['mae'],
                        'rmse': result['rmse'],
                        'r2': result['r2']
                    }
                    for name, result in model_results.items()
                }
            }
            results['analysis_components'].append('model_training')
        
        # Generate predictions
        print("\nüîÆ Step 3: Generating air quality predictions...")
        predictions = self.predict_air_quality(hours_ahead=48)
        if 'error' not in predictions:
            results['predictions'] = predictions
            results['analysis_components'].append('air_quality_forecasting')
        
        # Seasonal analysis
        print("\nüå± Step 4: Analyzing seasonal patterns...")
        seasonal_analysis = self.analyze_seasonal_patterns()
        if 'error' not in seasonal_analysis:
            results['seasonal_analysis'] = seasonal_analysis
            results['analysis_components'].append('seasonal_pattern_analysis')
        
        # Health impact analysis
        print("\nüè• Step 5: Generating health impact report...")
        health_report = self.generate_health_impact_report()
        if 'error' not in health_report:
            results['health_impact'] = health_report
            results['analysis_components'].append('health_impact_modeling')
        
        # Save comprehensive report
        print("\nüíæ Step 6: Saving comprehensive report...")
        report_file = self.output_dir / f'predictive_analytics_report_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
        with open(report_file, 'w') as f:
            json.dump(results, f, indent=2, default=str)
        
        print("\n‚úÖ Complete predictive analytics analysis finished!")
        print(f"üìä Report saved to: {report_file}")
        print(f"üîß Components completed: {', '.join(results['analysis_components'])}")
        
        return results

def main():
    """Main execution function for standalone testing."""
    print("ü§ñ Hot Durham Predictive Analytics System")
    print("=" * 50)
    
    # Initialize system
    analytics = PredictiveAnalytics()
    
    # Run complete analysis
    results = analytics.run_complete_analysis()
    
    if 'error' not in results:
        print("\nüéâ Analysis completed successfully!")
        print(f"üìà Components: {len(results['analysis_components'])}")
        
        if 'predictions' in results and 'predictions' in results['predictions']:
            next_24h = [p for p in results['predictions']['predictions'] if p['hours_ahead'] <= 24]
            if next_24h:
                avg_predicted = np.mean([p['predicted_pm25'] for p in next_24h])
                print(f"üîÆ 24-hour average predicted PM2.5: {avg_predicted:.1f} Œºg/m¬≥")
    else:
        print(f"‚ùå Analysis failed: {results['error']}")

if __name__ == "__main__":
    main()
