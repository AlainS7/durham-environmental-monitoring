name: Daily Ingestion (Cloud Run Job)

on:
  schedule:
    - cron: '45 6 * * *' # 06:45 UTC daily: ingest -> embedded merge -> checks
  workflow_dispatch:
    inputs:
      job_name:
        description: 'Cloud Run Job name'
        required: false
        default: weather-data-uploader
      region:
        description: 'Region'
        required: false
        default: us-east1
      start_date:
        description: 'Start date (YYYY-MM-DD) for historical range (optional)'
        required: false
      end_date:
        description: 'End date (YYYY-MM-DD) for historical range (optional)'
        required: false
      date:
        description: 'Single date (YYYY-MM-DD) override (mutually exclusive with range)'
        required: false
      redeploy_image:
        description: 'Set to true to build & push latest container image before running job'
        required: false
        default: 'false'
      backfill_merge:
        description: 'Set to true to run merge_backfill_range after ingestion (range or single)'
        required: false
        default: 'true'
      run_checks:
        description: 'Set to true to run staging presence & freshness checks after merge'
        required: false
        default: 'true'

env:
  PROJECT_ID: durham-weather-466502

permissions:
  contents: read
  id-token: write

jobs:
  ingest:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Auth to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          project_id: ${{ env.PROJECT_ID }}
          workload_identity_provider: '${{ secrets.GCP_WORKLOAD_IDENTITY_PROVIDER }}'
          service_account: '${{ secrets.GCP_VERIFIER_SA }}'

      - name: Validate required secrets (fail-fast)
        run: |
          set -euo pipefail
          missing=()
          req=(GCP_WORKLOAD_IDENTITY_PROVIDER GCP_VERIFIER_SA DB_CREDS_SECRET_ID TSI_CREDS_SECRET_ID WU_API_KEY_SECRET_ID)
          for k in "${req[@]}"; do
            if [ -z "${!k:-}" ]; then
              missing+=("$k")
            fi
          done
          if [ ${#missing[@]} -gt 0 ]; then
            echo "Missing required secrets: ${missing[*]}" >&2
            exit 1
          fi
          echo "All required secrets present: ${req[*]}"
        env:
          GCP_WORKLOAD_IDENTITY_PROVIDER: ${{ secrets.GCP_WORKLOAD_IDENTITY_PROVIDER }}
          GCP_VERIFIER_SA: ${{ secrets.GCP_VERIFIER_SA }}
          DB_CREDS_SECRET_ID: ${{ secrets.DB_CREDS_SECRET_ID }}
          TSI_CREDS_SECRET_ID: ${{ secrets.TSI_CREDS_SECRET_ID }}
          WU_API_KEY_SECRET_ID: ${{ secrets.WU_API_KEY_SECRET_ID }}

      - name: Setup gcloud
        uses: google-github-actions/setup-gcloud@v2

      - name: Build & Push container image (optional)
        if: ${{ github.event.inputs.redeploy_image == 'true' }}
        env:
          PROJECT_ID: ${{ env.PROJECT_ID }}
        run: |
          IMAGE="us-east1-docker.pkg.dev/$PROJECT_ID/weather-data-images/weather-data-uploader:git-${GITHUB_SHA}"
          echo "Building image $IMAGE"
          gcloud builds submit --tag "$IMAGE" .
          echo "IMAGE_URI=$IMAGE" >> $GITHUB_ENV

      - name: Update Cloud Run job image (optional)
        if: ${{ github.event.inputs.redeploy_image == 'true' }}
        env:
          PROJECT_ID: ${{ env.PROJECT_ID }}
          JOB_NAME: ${{ github.event.inputs.job_name || 'weather-data-uploader' }}
        run: |
          if [ -z "${IMAGE_URI:-}" ]; then
            echo "IMAGE_URI not set; previous step may have failed" >&2; exit 1; fi
          echo "Updating Cloud Run job $JOB_NAME to image $IMAGE_URI"
            gcloud run jobs update $JOB_NAME \
              --image "$IMAGE_URI" \
              --region "${{ github.event.inputs.region || 'us-east1' }}" \
              --project "$PROJECT_ID"

      - name: Execute ingestion job
        env:
          JOB_NAME: ${{ github.event.inputs.job_name || 'weather-data-uploader' }}
          REGION: ${{ github.event.inputs.region || 'us-east1' }}
          START_DATE: ${{ github.event.inputs.start_date }}
          END_DATE: ${{ github.event.inputs.end_date }}
          DATE: ${{ github.event.inputs.date }}
          # BigQuery env for staging writer inside container
          BQ_PROJECT: ${{ env.PROJECT_ID }}
          BQ_DATASET: sensors
          # Secret Manager IDs (the app will fetch JSON secrets at runtime)
          DB_CREDS_SECRET_ID: ${{ secrets.DB_CREDS_SECRET_ID }}
          TSI_CREDS_SECRET_ID: ${{ secrets.TSI_CREDS_SECRET_ID }}
          WU_API_KEY_SECRET_ID: ${{ secrets.WU_API_KEY_SECRET_ID }}
        run: |
          bash scripts/run_cr_job.sh

      - name: Merge backfill range (optional)
        if: ${{ github.event.inputs.backfill_merge == 'true' }}
        env:
          BQ_PROJECT: ${{ env.PROJECT_ID }}
        run: |
          set -euo pipefail
          START="${{ github.event.inputs.start_date }}"
          END="${{ github.event.inputs.end_date }}"
          SINGLE="${{ github.event.inputs.date }}"
          if [ -n "$SINGLE" ]; then
            START="$SINGLE"; END="$SINGLE"; fi
          if [ -z "$START" ] || [ -z "$END" ]; then
            echo "No start/end (and no single date) provided; skipping backfill merge"; exit 0; fi
          echo "Running per-source dated merge backfill $START -> $END"
          python scripts/merge_backfill_range.py \
            --project "$BQ_PROJECT" \
            --dataset sensors \
            --start "$START" \
            --end "$END" \
            --per-source-dated \
            --sources tsi,wu \
            --update-only-if-changed

      - name: Staging presence check (optional)
        if: ${{ github.event.inputs.run_checks == 'true' && github.event.inputs.date != '' }}
        env:
          BQ_PROJECT: ${{ env.PROJECT_ID }}
        run: |
          python scripts/check_staging_presence.py \
            --project "$BQ_PROJECT" \
            --dataset sensors \
            --date "${{ github.event.inputs.date }}" \
            --sources tsi,wu

      - name: Freshness check (optional)
        if: ${{ github.event.inputs.run_checks == 'true' }}
        env:
          BQ_PROJECT: ${{ env.PROJECT_ID }}
        run: |
          python scripts/check_freshness.py --project "$BQ_PROJECT" --dataset sensors --table sensor_readings --max-lag-days 1

      - name: Upload execution log (always)
        if: always()
        run: |
          echo "Ingestion workflow completed at $(date -u)" > ingestion_summary.log
          ls -1 *.log 2>/dev/null || true
        shell: bash

      - name: Artifact logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: ingestion-logs
          path: |
            *.log
            ingestion_summary.log
